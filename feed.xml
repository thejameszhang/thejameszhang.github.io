<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://thejameszhang.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://thejameszhang.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-12-22T06:28:09+00:00</updated><id>https://thejameszhang.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Tensor Factor Models</title><link href="https://thejameszhang.github.io/blog/2024/tfm/" rel="alternate" type="text/html" title="Tensor Factor Models"/><published>2024-12-03T00:00:00+00:00</published><updated>2024-12-03T00:00:00+00:00</updated><id>https://thejameszhang.github.io/blog/2024/tfm</id><content type="html" xml:base="https://thejameszhang.github.io/blog/2024/tfm/"><![CDATA[]]></content><author><name>James Zhang</name></author><category term="tensors"/><category term="characteristics"/><category term="pca"/><summary type="html"><![CDATA[Ongoing]]></summary></entry><entry><title type="html">Neural Tangent Kernels in Asset Pricing</title><link href="https://thejameszhang.github.io/blog/2024/ntk/" rel="alternate" type="text/html" title="Neural Tangent Kernels in Asset Pricing"/><published>2024-06-19T00:00:00+00:00</published><updated>2024-06-19T00:00:00+00:00</updated><id>https://thejameszhang.github.io/blog/2024/ntk</id><content type="html" xml:base="https://thejameszhang.github.io/blog/2024/ntk/"><![CDATA[<h1 id="tldr">TL;DR</h1> <p>Empircal characteristic-based asset pricing models have been widely used to explain differences in average returns of assets since Rosenberg, with the most classic example being the Fama French Three-Factor Model <d-cite key="ff3"></d-cite>. Asset pricing theory asserts that expected returns vary because assets themselves have varying exposure to the stochastic discount factor (SDF). Existing literature primarily implicitly assume that factor betas or risk prices are linear functions of pre-specified characteristics. However, a recent growing literature shows that nonlinear interactions between characteristics are important.</p> <p>It follows that researchers turn to deep neural networks (DNNs), but DNNs have long been thought of as a “black box” by researchers and industry experts alike. However, it’s become possible to study the evolution of infinite-width feedforward neural networks during training via gradient descent through a special kernel called the <strong>neural tangent kernel (NTK)</strong> <d-cite key="ntk"></d-cite>. Moreover, <d-cite key="ntk2"></d-cite> generalizes the NTK to any architecture whose forward and backpropagation can be expressed as a combination activation functions and matrix multiplications. With these two key results, I’d like to study the use of modern deep neural network architectures – autoencoders, transformers, and more – to estimate asset pricing models.</p>]]></content><author><name>James Zhang</name></author><category term="theory"/><summary type="html"><![CDATA[A rabbit hole into NTKs]]></summary></entry><entry><title type="html">Multimodal Reinforcement Learning Alpha</title><link href="https://thejameszhang.github.io/blog/2024/rl/" rel="alternate" type="text/html" title="Multimodal Reinforcement Learning Alpha"/><published>2024-05-20T17:39:00+00:00</published><updated>2024-05-20T17:39:00+00:00</updated><id>https://thejameszhang.github.io/blog/2024/rl</id><content type="html" xml:base="https://thejameszhang.github.io/blog/2024/rl/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[An alt data alpha report]]></summary></entry><entry><title type="html">The Kernel Trick</title><link href="https://thejameszhang.github.io/blog/2024/kernels/" rel="alternate" type="text/html" title="The Kernel Trick"/><published>2024-04-07T00:00:00+00:00</published><updated>2024-04-07T00:00:00+00:00</updated><id>https://thejameszhang.github.io/blog/2024/kernels</id><content type="html" xml:base="https://thejameszhang.github.io/blog/2024/kernels/"><![CDATA[<p>Thanks to my mentor Professor Serhiy Kozak for countless insightful discussions and the opportunity to contribute to the codebase for this paper. This blog post is based on <d-cite key="Kozak2020Kernel"></d-cite>.</p> <h1 id="tldr">TL;DR</h1>]]></content><author><name>James Zhang</name></author><category term="kernel-methods"/><summary type="html"><![CDATA[A paper I contributed some code to]]></summary></entry><entry><title type="html">Conditional Asset Pricing Model for Global Equities</title><link href="https://thejameszhang.github.io/blog/2024/globaleq/" rel="alternate" type="text/html" title="Conditional Asset Pricing Model for Global Equities"/><published>2024-01-25T00:00:00+00:00</published><updated>2024-01-25T00:00:00+00:00</updated><id>https://thejameszhang.github.io/blog/2024/globaleq</id><content type="html" xml:base="https://thejameszhang.github.io/blog/2024/globaleq/"><![CDATA[<p>Thanks to my mentor Professor Serhiy Kozak for countless insightful discussions and the opportunity to contribute to the codebase for this paper. This blog post is based on <d-cite key="csapspan"></d-cite>.</p> <h1 id="tldr">TL;DR</h1> <p>Firm characteristics are the primary pricing signals for stock returns in cross-sectional asset pricing research. Researchers often use these characteristics to create characteristic-based factors, which are long-short zero-cost portfolios whose asset weights are functions of firm characteristics. They then use these factors to construct Stochastic Discount Factor (SDF) proxies that summarize the cross-section. The most famous of these multifactor SDFs is perhaps the Fama-French 3 Factor Model <d-cite key="ff3"></d-cite>, which use decile cutoffs on size, book-to-market ratios, and market \(\beta\) to construct 3 characteristic-based factors. We denote these factors that follow from sorting stocks by firm characteristics as <em>sorted factors</em>. Other methods of factor construction include <em>univariate factors</em><sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>, <em>OLS factors</em> <sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>, and <em>SVD factors</em> <sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>.</p> <p>In the paper, Kozak and Nagel discuss three main things in the paper:</p> <ol> <li> <p>Recall that the covariance matrix of individual stock returns is imperative for computing the mean-variance efficient portfolio, yet none of these heuristic factor construction methods use it in their formulations. Thus, if an econometrician chooses to construct such a multifactor model by aggregating stocks into characteristic-based portfolios, in what conditions does a mean-variance efficient portfolio of the factors yield the mean-variance efficient portfolio of individual stocks, or as they word it, “Under which conditions is the investment opportunity set not deteriorating if one aggregates individual assets to these factor portfolios?”.</p> </li> <li> <p>Furthermore, Kozak and Nagel establish the conditions in which hedge portfolios that remove unpriced risks in conjunction with the factors can span the SDF.</p> </li> <li> <p>Finally, relating the paper to the extensive literature on dimensionality reduction methods, we’re also curious to establish the conditions in which latent factors span the SDF</p> </li> </ol> <p>This theoretical paper establishes the conditions in which many heuristic methods construct factors that span the SDF. However, they impose that asset weights in characteristic-based portfolios are linear. In the rest of the post, we explore the performance of factors with weights that are nonlinear in characteristics.</p> <h1 id="the-linearity-assumption">The Linearity Assumption</h1> <p>Aside from sorted factors, the aforementioned heuristic methods also imposed asset weights in characteristic-based factors as linear functions of firm characteristics. This assumption that imposes linearity linking firm characteristics to expected returns has economic content. That is, we’ve previously observed functions \(\phi\) that are linear.</p> \[F_{t+1} = \phi(X_t)^T R_{t+1}\] <p>However, there exist present value identities that describe nonlinear relationships between certain characteristics and returns. Thus, the idea that expected returns and covariances are linear in \(X_t\), which is assumed in the paper, is not realistic, so we can assume neural networks to learn a more flexible nonlinear relationship instead. That is, with a model for returns of stock \(i\) at time \(t+1\)</p> \[r_{i, t+1} = \beta(X_{i, t}) F_{t+1} + \epsilon_{i, t+1}\] \[r_{i, t+1} = \beta(X_{i, t}) \phi(X_t)^T R_{t+1} + \epsilon_{i, t+1}\] <p>Let \(\phi(\cdot): \mathbb{R}^J \to \mathbb{R}^K\) map \(J\) original characteristics to \(K\) “transformed” characteristics. This same mapping applies at every time period \(t\) for every stock \(i\), so the input for the NN is just a sequence of \(J\) characteristics for a given stock at a given time; the output is a set of \(K\) transformed characteristics. Essentially, we seek \(\phi(\cdot)\) that minimizes MSE of the model.</p> <h1 id="implementation">Implementation</h1> <p><em>Note: Much of my research code is not publicly available per my mentor’s request, but please reach out via email to discuss anything.</em></p> <p>While optimizations like <code class="language-plaintext highlighter-rouge">jit</code> and <code class="language-plaintext highlighter-rouge">vmap</code> are included in PyTorch, all of the code for this project is written using JAX and Equinox, the latter being a deep learning library built on top of JAX. See this <a href="https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/JAX/tutorial2/Introduction_to_JAX.html">guide</a> by Phillip Lippe that benchmarks Jax and PyTorch – essentially, JAX can be extremely fast, and we use it for almost all of our projects.</p> <h2 id="equinox-and-neural-networks">Equinox and Neural Networks</h2> <p>To define a simple feedforward neural network using Equinox, one just has to specify the details of the architecture in the <code class="language-plaintext highlighter-rouge">__init__</code> function of the class that inherits from <code class="language-plaintext highlighter-rouge">eqx.Module</code>. That is, as a baseline, in <code class="language-plaintext highlighter-rouge">self.layers</code>, we have a sequence of <code class="language-plaintext highlighter-rouge">eqx.nn.Linear</code> layers followed by activation functions. It’s also customary to utilize the <code class="language-plaintext highlighter-rouge">__call__</code> method in the Equinox framework for cleaner code. In this method, essentially just apply all of the layers in the architecture to the input.</p> <p>In our NN, we also include batch normalization layers and a layer that orthonormalizes the transformed characteristics in the cross-section at every time \(t\). We test sigmoid, relu, and tanh activation functions and cross-validate them. Similar for the value of \(K\) – the output layer of the NN and the number of transformed characteristics – we try various options and cross-validate over them. In our <code class="language-plaintext highlighter-rouge">__call__</code> method, we also use <code class="language-plaintext highlighter-rouge">jax.vmap</code> on the linear layers to apply the same NN transformation in the cross-section ie. to all stocks at once. The performance speedup with this design choice is significant. When training the model, in the heart of batch gradient descent, we can again leverage <code class="language-plaintext highlighter-rouge">jax.vmap</code> to vector map across the time dimension.</p> <h1 id="conclusions">Conclusions</h1> <p>Without any iterative hedging methods to incorporate more elements of the covariance matrix of stock returns into the factors, our NN transformation is an improvement over the aforementioned linear transformations rotating characteristics into characteristic-based portfolios. Below are some very preliminary results. We report OOS \(R^2\) on our large tensor dataset of around \(1900\) stocks.</p> <ul> <li>Univariate Factors \(R^2\): \(0.20930\)</li> <li>OLS Factors \(R^2\): \(0.20930\)</li> <li>SVD Factors \(R^2\): \(0.20956\)</li> <li>NN Factors \(R^2\): \(0.21102\)</li> </ul> <p>It would be interesting to further explore if iterative hedging methods or dimension reduction methods further improve OOS performance following a NN transformation. The NN transformation is good at fitting second moments, but not so good at modeling alphas. More work on the theory side needs to be done and try and figure out why this is.</p> <p>Thanks for reading. Feel free to reach out if you have any questions :)</p> <hr/> <h1 id="footnotes">Footnotes</h1> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p>The idea of univariate factors follows from the idea that true value of observable characteristics are less important in summarizing the cross-section than the relative ranking of stocks by their characteristic values. Consider the following example where we want to create a sorted factor on some characteristic \(x\). Suppose at time \(t\), the observable values of \(x\) for stocks \(A, B\), and \(C\) are \(10, 2, 8\), respectively. The relative ranking yields \(3, 1, 2\). Demean this new list and divide all entries by the new sum of absolute entries. This yields \(0.5, -0.5, 0\) as the asset weights constructing the factor sorted on \(x\) Here, asset weights are ranked characteristics, and betas are OLS weights; see below. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:2" role="doc-endnote"> <p>To construct OLS factors, let \(X_t \in \mathbb{R}^{N \times J}\) be a panel of \(J\) observed characteristics for \(N\) stocks. Let \(R_{t+1} \in \mathbb{R}^{N \times D}\) denote monthly excess returns. \(F_{t+1}\) has the generic form \(F_{t+1} = \phi(X_t)^TR_{t+1}\in \mathbb{R}^J\). In typical OLS fashion, this yields \(F_{t+1} = (X_t^T X_t)^{-1} X_t^T R_{t+1}\). Since we are interested in uncovering conditional betas \(\beta_t \in \mathbb{R}^{N \times J}\) and \(F_{t+1}\) that solve \(R_{t+1} = \beta_t F_{t+1}\). By substitution for \(F_{t+1}\), \(R_{t+1} = \beta_t (X_t^TX_t)^{-1} X_t^T R_{t+1}\). This implies that \(\beta_t (X_t^T X_t)^{-1} X_t^T = I \implies \beta_t = X_t\) in this construction, so betas are exactly characteristcs. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:3" role="doc-endnote"> <p>Here, \(F_{t+1} = (X_t^T X_t)^{-1/2} X_t^T R_{t+1}\). By substitution, we seek betas that satisfy \(R_{t+1} = \beta_t (X_t^T X_t)^{-1/2} X_t^T R_{t+1}\), which therefore implies that \(\beta_t (X_t^T X_t)^{-1/2} X_t^T = I\). Thus, it follows that \(\beta_t (X_t^T X_t)^{-1/2} (X_t^T X_t) = X_t\). Taking inverses, \(\beta_t = X_t (X_t^T X_t)^{-1} (X_t^T X_t)^{1/2} = X_t (X_t^T X_t)^{-1/2}\), and so in this construction, betas and asset weights in the factors are equal (the transpose of one another). <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name>James Zhang</name></author><category term="equities"/><category term="neural-networks"/><summary type="html"><![CDATA[A paper I contributed some code to]]></summary></entry><entry><title type="html">Equities Pairs Trading</title><link href="https://thejameszhang.github.io/blog/2023/pairs/" rel="alternate" type="text/html" title="Equities Pairs Trading"/><published>2023-07-16T00:00:00+00:00</published><updated>2023-07-16T00:00:00+00:00</updated><id>https://thejameszhang.github.io/blog/2023/pairs</id><content type="html" xml:base="https://thejameszhang.github.io/blog/2023/pairs/"><![CDATA[ <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/pairs.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div> ]]></content><author><name></name></author><category term="sif"/><category term="trading"/><category term="datascience"/><summary type="html"><![CDATA[A research project enhancing a famous algo trading strategy]]></summary></entry></feed>