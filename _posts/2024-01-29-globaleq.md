---
layout: distill
title: Neural Networks in Factor Portfolio Construction 
description: A paper I contributed some code to
tags: equities neural-networks
giscus_comments: # true
date: 2024-01-25
# featured: true
mermaid:
  enabled: true
  zoomable: true
code_diff: true
map: true
chart:
  chartjs: true
  echarts: true
  vega_lite: true
tikzjax: true
typograms: true

authors:
  - name: James Zhang
    url: "thejameszhang.github.io"
    affiliations:
      name: University of Maryland, College Park

bibliography: globaleq.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
# toc:
#   - name: Equations
#     # if a section has subsections, you can add them as follows:
#     # subsections:
#     #   - name: Example Child Subsection 1
#     #   - name: Example Child Subsection 2
#   - name: Citations
#   - name: Footnotes

# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

Thanks to my mentor Professor Serhiy Kozak for countless insightful discussions and the opportunity to contribute to the codebase for this paper. This blog post is based on <d-cite key="csapspan"></d-cite>.

<!-- The citation is presented inline like this: <d-cite key="csapspan"></d-cite> (a number that displays more information on hover).
If you have an appendix, a bibliography is automatically created and populated in it. -->

# TL;DR


Firm characteristics are the primary pricing signals for stock returns in cross-sectional asset pricing research. Researchers often use these characteristics to create characteristic-based factors, which are long-short zero-cost portfolios whose asset weights are functions of firm characteristics. They then use these factors to construct Stochastic Discount Factor (SDF) proxies that summarize the cross-section. The most famous of these multifactor SDFs is perhaps the Fama-French 3 Factor Model <d-cite key="ff3"></d-cite>, which use decile cutoffs on size, book-to-market ratios, and market $$\beta$$ to construct 3 characteristic-based factors. We denote these factors that follow from sorting stocks by firm characteristics as *sorted factors*. Other methods of factor construction include *univariate factors*[^1], *OLS factors* [^2], and *SVD factors* [^3].

[^1]: The idea of univariate factors follows from the idea that true value of observable characteristics are less important in summarizing the cross-section than the relative ranking of stocks by their characteristic values. Consider the following example where we want to create a sorted factor on some characteristic $$x$$. Suppose at time $$ t $$, the observable values of $$x$$ for stocks $$A, B$$, and $$C$$ are $$10, 2, 8$$, respectively. The relative ranking yields $$3, 1, 2$$. Demean this new list and divide all entries by the new sum of absolute entries. This yields $$0.5, -0.5, 0$$ as the asset weights constructing the factor sorted on $$x$$ Here, asset weights are ranked characteristics, and betas are OLS weights; see below.

[^2]: To construct OLS factors, let $$X_t \in \mathbb{R}^{N \times J}$$ be a panel of $$J$$ observed characteristics for $$N$$ stocks. Let $$R_{t+1} \in \mathbb{R}^{N \times D}$$ denote monthly excess returns. $$F_{t+1}$$ has the generic form $$F_{t+1} = \phi(X_t)^TR_{t+1}\in \mathbb{R}^J$$. In typical OLS fashion, this yields $$F_{t+1} = (X_t^T X_t)^{-1} X_t^T R_{t+1}$$. Since we are interested in uncovering conditional betas $$\beta_t \in \mathbb{R}^{N \times J}$$ and $$F_{t+1}$$ that solve $$R_{t+1} = \beta_t F_{t+1}$$. By substitution for $$F_{t+1}$$, $$R_{t+1} = \beta_t (X_t^TX_t)^{-1} X_t^T R_{t+1}$$. This implies that $$\beta_t (X_t^T X_t)^{-1} X_t^T = I \implies \beta_t = X_t$$ in this construction, so betas are exactly characteristcs.

[^3]: Here, $$F_{t+1} = (X_t^T X_t)^{-1/2} X_t^T R_{t+1}$$. By substitution, we seek betas that satisfy $$R_{t+1} = \beta_t (X_t^T X_t)^{-1/2} X_t^T R_{t+1}$$, which therefore implies that $$\beta_t (X_t^T X_t)^{-1/2} X_t^T = I$$. Thus, it follows that $$\beta_t (X_t^T X_t)^{-1/2} (X_t^T X_t) = X_t$$. Taking inverses, $$\beta_t = X_t (X_t^T X_t)^{-1} (X_t^T X_t)^{1/2} = X_t (X_t^T X_t)^{-1/2}$$, and so in this construction, betas and asset weights in the factors are equal (the transpose of one another).

In the paper, Kozak and Nagel discuss three main things in the paper: 

1. Recall that the covariance matrix of individual stock returns is imperative for computing the mean-variance efficient portfolio, yet none of these heuristic factor construction methods use it in their formulations. Thus, if an econometrician chooses to construct such a multifactor model by aggregating stocks into characteristic-based portfolios, in what conditions does a mean-variance efficient portfolio of the factors yield the mean-variance efficient portfolio of individual stocks, or as they word it, "Under which conditions is the investment opportunity set not deteriorating if one aggregates individual assets to these factor portfolios?".

2. Furthermore, Kozak and Nagel establish the conditions in which hedge portfolios that remove unpriced risks in conjunction with the factors can span the SDF.

3. Finally, relating the paper to the extensive literature on dimensionality reduction methods, we're also curious to establish the conditions in which latent factors span the SDF

This theoretical paper establishes the conditions in which many heuristic methods construct factors that span the SDF. However, they impose that asset weights in characteristic-based portfolios are linear. In the rest of the post, we explore the performance of factors with weights that are nonlinear in characteristics.  

# The Linearity Assumption

Aside from sorted factors, the aforementioned heuristic methods also imposed asset weights in characteristic-based factors as linear functions of firm characteristics. This assumption that imposes linearity linking firm characteristics to expected returns has economic content. That is, we've previously observed functions $$\phi$$ that are linear. 

$$F_{t+1} = \phi(X_t)^T R_{t+1}$$

However, there exist present value identities that describe nonlinear relationships between certain characteristics and returns. Thus, the idea that expected returns and covariances are linear in $$X_t$$, which is assumed in the paper, is not realistic, so we can assume neural networks to learn a more flexible nonlinear relationship instead. That is, with a model for returns of stock $$i$$ at time $$t+1$$

$$r_{i, t+1} = \beta(X_{i, t}) F_{t+1} + \epsilon_{i, t+1}$$

$$r_{i, t+1} = \beta(X_{i, t}) \phi(X_t)^T R_{t+1} + \epsilon_{i, t+1}$$

Let $$\phi(\cdot): \mathbb{R}^J \to \mathbb{R}^K$$ map $$J$$ original characteristics to $$K$$ "transformed" characteristics. This same mapping applies at every time period $$t$$ for every stock $$i$$, so the input for the NN is just a sequence of $$J$$ characteristics for a given stock at a given time; the output is a set of $$K$$ transformed characteristics. Essentially, we seek $$\phi(\cdot)$$ that minimizes MSE of the model.

# Implementation

*Note: Much of my research code is not publicly available per my mentorâ€™s request, but please reach out via email to discuss anything.*

While optimizations like `jit` and `vmap` are included in PyTorch, all of the code for this project is written using JAX and Equinox, the latter being a deep learning library built on top of JAX. See this [guide](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/JAX/tutorial2/Introduction_to_JAX.html) by Phillip Lippe that benchmarks Jax and PyTorch -- essentially, JAX can be extremely fast, and we use it for almost all of our projects.

## Equinox and Neural Networks

To define a simple feedforward neural network using Equinox, one just has to specify the details of the architecture in the `__init__` function of the class that inherits from `eqx.Module`. That is, as a baseline, in `self.layers`, we have a sequence of `eqx.nn.Linear` layers followed by activation functions. It's also customary to utilize the `__call__` method in the Equinox framework for cleaner code. In this method, essentially just apply all of the layers in the architecture to the input.

In our NN, we also include batch normalization layers and a layer that orthonormalizes the transformed characteristics in the cross-section at every time $$t$$. We test sigmoid, relu, and tanh activation functions and cross-validate them. Similar for the value of $$K$$ -- the output layer of the NN and the number of transformed characteristics -- we try various options and cross-validate over them. In our `__call__` method, we also use `jax.vmap` on the linear layers to apply the same NN transformation in the cross-section ie. to all stocks at once. The performance speedup with this design choice is significant. When training the model, in the heart of batch gradient descent, we can again leverage `jax.vmap` to vector map across the time dimension. 

# Conclusions

Without any iterative hedging methods to incorporate more elements of the covariance matrix of stock returns into the factors, our NN transformation is an improvement over the aforementioned linear transformations rotating characteristics into characteristic-based portfolios. Below are some very preliminary results. We report OOS $$R^2$$ on our large tensor dataset of around $$1900$$ stocks. 

- Univariate Factors $$R^2$$: $$0.20930$$
- OLS        Factors $$R^2$$: $$0.20930$$
- SVD        Factors $$R^2$$: $$0.20956$$
- NN         Factors $$R^2$$: $$0.21102$$

It would be interesting to further explore if iterative hedging methods or dimension reduction methods further improve OOS performance following a NN transformation. The NN transformation is good at fitting second moments, but not so good at modeling alphas. More work on the theory side needs to be done and try and figure out why this is. 

Thanks for reading. Feel free to reach out if you have any questions :)

--- 
# Footnotes



